# Config for a simple 256 -> 16 autoencoder
model:
  model_id: titok
  sample_size: 16
  channels: 32
  latent_size: 16
  latent_channels: 128
  
  noise_decoder_inputs: 0.05
  
  n_layers: 12
  n_heads: 6
  d_model: 384

  patch_size: 1

train:
  trainer_id: proxy
  data_id: local_imagenet_256

  target_batch_size: 32
  batch_size: 32

  epochs: 200

  opt: Muon
  opt_kwargs:
    lr: 1.0e-3
    momentum: 0.95
    adamw_lr: 1.0e-4
    adamw_wd: 0.01
    adamw_eps: 1.0e-15
    adamw_betas: [0.9, 0.95]
    adamw_keys: [encoder.proj_in, encoder.proj_out, decoder.proj_in, decoder.proj_out]

  loss_weights:
    latent_reg: 1.0e-5
    lpips: 0.0

  scheduler: LinearWarmup
  scheduler_kwargs:
    warmup_steps: 3000
    min_lr: 1.0e-5

  checkpoint_dir: checkpoints/2d_proxy

  sample_interval: 1000
  save_interval: 10000

  teacher_ckpt: checkpoints/2d_16x/step_30000/model.pt
  teacher_cfg: configs/2d_16x.yml

wandb:
  name: shahbuland
  project: new_vaes
  run_name: 64x_imagenet